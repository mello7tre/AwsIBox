#!/usr/bin/env python3
import logging
import boto3
from math import floor
from pprint import pformat

from datetime import datetime, timezone
from operator import itemgetter

logging.basicConfig()
logging.getLogger("botocore").setLevel("CRITICAL")
logger = logging.getLogger("ibox_cluster_autoscale")
logger.setLevel(logging.INFO)

CLIENT_ECS = boto3.client("ecs")
CLIENT_AUTOSCALING = boto3.client("autoscaling")

IS_LAMBDA = True
DO_MANUAL_REDUCE = False

N_REDUCE_FOR_RUN = 3
N_PASS_SKIP_COUNT = 2
N_SKIP_INSTANCE_SECONDS = 150


def sort_container_instances(containerInstances, AZones):
    # add the total number of instance in the same zone
    for n in containerInstances:
        n["nAZone"] = AZones.get(n["AZone"], 0)

    # and sort first by runningTasksCount and registeredAt asc then nAZone desc
    # this way the candidate to terminate is the instance in the zone with
    # the greater number of instance running (primary condition)  with the less number of tasks and older (secondary ones)
    i_sorted = sorted(
        containerInstances, key=itemgetter("runningTasksCount", "registeredAt")
    )
    i_sorted = sorted(i_sorted, key=itemgetter("nAZone"), reverse=True)

    return i_sorted


def process_containers(cluster, containers, res):
    net = 0
    response = CLIENT_ECS.describe_container_instances(
        cluster=cluster, containerInstances=containers
    )
    AZones = {}
    containerInstances = []

    for n in response["containerInstances"]:
        az = ""
        for a in n["attributes"]:
            if a["name"] == "ecs.availability-zone":
                az = a["value"]
                AZones[az] = AZones.get(az, 0) + 1
                continue
        for r in n["registeredResources"]:
            if r["name"] == "CPU":
                res["cpu_base"] = r["integerValue"]
                net += res["cpu_base"] / 1024 * 5
            if r["name"] == "MEMORY":
                res["ram_base"] = r["integerValue"]
        for r in n["remainingResources"]:
            if r["name"] == "CPU":
                res["cpu_free"] += r["integerValue"]
            if r["name"] == "MEMORY":
                res["ram_free"] += r["integerValue"]

        registered_at = n["registeredAt"]
        ec2_instance_id = n["ec2InstanceId"]
        registered_seconds_ago = datetime.now(timezone.utc) - registered_at
        if registered_seconds_ago.total_seconds() < N_SKIP_INSTANCE_SECONDS:
            # skip istance registered less than N_SKIP_INSTANCE_SECONDS
            logger.info(
                f"Cluster: {cluster}, Instance: {ec2_instance_id} registered at {registered_at} "
                f"is too young and will be skipped {registered_seconds_ago.total_seconds()} < {N_SKIP_INSTANCE_SECONDS}"
            )
            continue

        containerInstances.append(
            {
                "runningTasksCount": n["runningTasksCount"],
                "registeredAt": registered_at,
                "ec2InstanceId": ec2_instance_id,
                "AZone": az,
            }
        )

    i_len = len(containerInstances)
    for _ in range(0, min(i_len, N_REDUCE_FOR_RUN)):
        i_sorted = sort_container_instances(containerInstances, AZones)
        i = i_sorted.pop(0)
        res["i_tasks"].append(i["runningTasksCount"])
        res["i_id"].append(i["ec2InstanceId"])
        res["i_date"].append(i["registeredAt"])
        res["i_AZ"].append(i["AZone"])
        res["n_i_in_AZ"].append(i["nAZone"])
        # remove processed instance
        containerInstances.remove(i)
        # and reduce relative AZone
        AZones[i["AZone"]] -= 1

    res["net_free"] = net - res["net_used"]


def populate_free_resources(cluster, res):
    ci_active = 0
    paginator = CLIENT_ECS.get_paginator("list_container_instances")
    response_iterator = paginator.paginate(cluster=cluster, status="ACTIVE")
    for s in response_iterator:
        container_instance_arns = s.get("containerInstanceArns", [])
        ci_active += len(container_instance_arns)
        # describe_container_instances - only max 100 containerInstances
        while container_instance_arns:
            cia = container_instance_arns[0:100]
            process_containers(cluster, cia, res)
            del container_instance_arns[0:100]

    # update n_instances to take in account only ACTIVE Container Instances (more conservative mode)
    res["n_instances"] = ci_active
    return res


def process_services(c):
    net = 0
    paginator_service = CLIENT_ECS.get_paginator("list_services")
    response_iterator_service = paginator_service.paginate(
        cluster=c,
        launchType="EC2",
        PaginationConfig={
            "PageSize": 10,
        },
    )
    for s in response_iterator_service:
        if not s["serviceArns"]:
            continue
        s_d = CLIENT_ECS.describe_services(
            cluster=c,
            services=s["serviceArns"],
        )["services"]
        for srv in s_d:
            if srv.get("networkConfiguration"):
                net += srv["runningCount"] + srv["pendingCount"]

    return net


def need_to_run(c):
    enabled = False
    run = False
    count = 0

    for n in c["tags"]:
        key = n["key"]
        value = n["value"]
        if key == "IBOX_CLUSTER_AUTO_REDUCE":
            enabled = True
            if value == "yes":
                run = True
        if key == "IBOX_CLUSTER_AUTO_REDUCE_COUNT":
            count = int(value)
    if run:
        for n in c["statistics"]:
            name = n["name"]
            value = n["value"]
            if name == "pendingEC2TasksCount" and int(value) > 0:
                run = False

    n_instances = c["registeredContainerInstancesCount"]
    if n_instances == 0:
        run = False

    return enabled, run, count, n_instances


def tag_ecs_count(cluster, c):
    CLIENT_ECS.tag_resource(
        resourceArn=cluster,
        tags=[{"key": "IBOX_CLUSTER_AUTO_REDUCE_COUNT", "value": str(c)}],
    )


def terminate_instance(instance, cluster):
    logger.info(f"Terminate {instance}")
    try:
        CLIENT_AUTOSCALING.terminate_instance_in_auto_scaling_group(
            InstanceId=instance, ShouldDecrementDesiredCapacity=True
        )
    except Exception as e:
        logger.info(e)
    tag_ecs_count(cluster, 0)


def process_cluster(c):
    cluster = c["clusterArn"]

    enabled, run, count, n_instances = need_to_run(c)

    if not run and IS_LAMBDA:
        if count > 0 and enabled:
            tag_ecs_count(cluster, 0)
        return

    c_name = cluster.split("/")[1]

    res = {
        "cluster": c_name,
        "n_instances": n_instances,
        "count": count,
        "i_date": [],
        "i_id": [],
        "i_tasks": [],
        "i_AZ": [],
        "n_i_in_AZ": [],
        #        "i_date": datetime.now(timezone.utc),
        #        "i_id": None,
        #        "i_tasks": 2000000,
        "cpu_free": 0,
        "ram_free": 0,
        "net_free": 0,
        "net_base": 0,
        "net_used": 0,
        "cpu_base": 0,
        "ram_base": 0,
    }
    net = process_services(cluster)
    res["net_used"] += net
    populate_free_resources(cluster, res)
    res["net_base"] = res["cpu_base"] / 1024 * 5

    logger.info(pformat(res))

    if n_instances == 0:
        return

    cpu_extra = floor(res["cpu_free"] / res["cpu_base"])
    ram_extra = floor(res["ram_free"] / res["ram_base"])
    net_extra = floor(res["net_free"] / res["net_base"])

    n_reduce = min(cpu_extra, ram_extra, net_extra)
    logger.info(f"Cluster {c_name} can be reduced of {n_reduce}")

    if not IS_LAMBDA and not DO_MANUAL_REDUCE:
        return

    if cpu_extra > 0 and ram_extra > 0 and net_extra > 0:
        count += 1
        is_terminate_cycle = True if count >= N_PASS_SKIP_COUNT else False
        if is_terminate_cycle or 0 in res["i_tasks"]:
            # if no tasks are running can terminate instance on the first cycle
            for n in range(0, min(n_reduce, len(res["i_id"]))):
                if is_terminate_cycle or res["i_tasks"][n] == 0:
                    terminate_instance(res["i_id"][n], cluster)
        else:
            tag_ecs_count(cluster, count)
            logger.info(f"Skip terminate count={count}")
    else:
        tag_ecs_count(cluster, 0)


def lambda_handler(event=None, context=None):
    # warning max 100 clusters are returned
    clusters_list = CLIENT_ECS.list_clusters()["clusterArns"]

    clusters = CLIENT_ECS.describe_clusters(
        clusters=clusters_list, include=["TAGS", "STATISTICS"]
    )["clusters"]

    for c in clusters:
        process_cluster(c)
