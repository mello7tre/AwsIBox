#!/usr/bin/env python3
import logging
import boto3
import argparse
from math import floor
from pprint import pformat
from datetime import datetime, timezone

logging.basicConfig()
logging.getLogger("botocore").setLevel("CRITICAL")
logger = logging.getLogger("ibox_cluster_autoscale")
logger.setLevel(logging.INFO)

CLIENT_ECS = boto3.client("ecs")
CLIENT_AUTOSCALING = boto3.client("autoscaling")


# parse main argumets
def get_args():
    parser = argparse.ArgumentParser(
        description="Autoscale ECS Cluster looking at running tasks"
    )

    parser.add_argument(
        "--clusters",
        "-c",
        help="Cluster to process - default all",
        default=[],
        nargs="+",
    )
    parser.add_argument("--reduce", "-r", help="Reduce Cluster", action="store_true")

    args = parser.parse_args()
    return args


def process_containers(cluster, containers, res):
    net = 0
    response = CLIENT_ECS.describe_container_instances(
        cluster=cluster, containerInstances=containers
    )

    for n in response["containerInstances"]:
        if n["registeredAt"] < res["i_date"]:
            res["i_date"] = n["registeredAt"]
            res["i_id"] = n["ec2InstanceId"]
        for r in n["registeredResources"]:
            if r["name"] == "CPU":
                res["cpu_base"] = r["integerValue"]
                net += res["cpu_base"] / 1024 * 5
            if r["name"] == "MEMORY":
                res["ram_base"] = r["integerValue"]
        for r in n["remainingResources"]:
            if r["name"] == "CPU":
                res["cpu_free"] += r["integerValue"]
            if r["name"] == "MEMORY":
                res["ram_free"] += r["integerValue"]

    res["net_free"] = net - res["net_used"]


def populate_free_resources(cluster, res):
    ci_active = 0
    paginator = CLIENT_ECS.get_paginator("list_container_instances")
    response_iterator = paginator.paginate(cluster=cluster, status="ACTIVE")
    for s in response_iterator:
        container_instance_arns = s["containerInstanceArns"]
        if not container_instance_arns:
            continue
        ci_active += len(container_instance_arns)
        # describe_container_instances - only max 100 containerInstances
        while container_instance_arns:
            cia = container_instance_arns[0:100]
            process_containers(cluster, cia, res)
            del container_instance_arns[0:100]

    # update n_instances to take in account only ACTIVE Container Instances (more conservative mode)
    res["n_instances"] = ci_active
    return res


def process_services(c):
    net = 0
    paginator_service = CLIENT_ECS.get_paginator("list_services")
    response_iterator_service = paginator_service.paginate(
        cluster=c,
        launchType="EC2",
        PaginationConfig={
            "PageSize": 10,
        },
    )
    for s in response_iterator_service:
        if not s["serviceArns"]:
            continue
        s_d = CLIENT_ECS.describe_services(
            cluster=c,
            services=s["serviceArns"],
        )["services"]
        for srv in s_d:
            if srv.get("networkConfiguration"):
                net += srv["runningCount"] + srv["pendingCount"]

    return net


def need_to_run(c):
    enabled = False
    run = False
    count = 0

    for n in c["tags"]:
        key = n["key"]
        value = n["value"]
        if key == "IBOX_CLUSTER_AUTO_REDUCE":
            enabled = True
            if value == "yes":
                run = True
        if key == "IBOX_CLUSTER_AUTO_REDUCE_COUNT":
            count = int(value)
    if run:
        for n in c["statistics"]:
            name = n["name"]
            value = n["value"]
            if name == "pendingEC2TasksCount" and int(value) > 0:
                run = False

    n_instances = c["registeredContainerInstancesCount"]
    if n_instances == 0:
        run = False

    return enabled, run, count, n_instances


def tag_ecs_count(cluster, c):
    CLIENT_ECS.tag_resource(
        resourceArn=cluster,
        tags=[{"key": "IBOX_CLUSTER_AUTO_REDUCE_COUNT", "value": str(c)}],
    )


def process_cluster(c):
    cluster = c["clusterArn"]

    enabled, run, count, n_instances = need_to_run(c)

    if not run and lambda_run:
        if count > 0 and enabled:
            tag_ecs_count(cluster, 0)
        return

    c_name = cluster.split("/")[1]

    res = {
        "cluster": c_name,
        "n_instances": n_instances,
        "count": count,
        "i_date": datetime.now(timezone.utc),
        "i_id": None,
        "cpu_free": 0,
        "ram_free": 0,
        "net_free": 0,
        "net_base": 0,
        "net_used": 0,
        "cpu_base": 0,
        "ram_base": 0,
    }
    net = process_services(cluster)
    res["net_used"] += net
    populate_free_resources(cluster, res)
    res["net_base"] = res["cpu_base"] / 1024 * 5

    logger.info(pformat(res))

    if n_instances == 0:
        return

    cpu_extra = floor(res["cpu_free"] / res["cpu_base"])
    ram_extra = floor(res["ram_free"] / res["ram_base"])
    net_extra = floor(res["net_free"] / res["net_base"])

    n_reduce = min(cpu_extra, ram_extra, net_extra)
    logger.info(f"Cluster {c_name} can be reduced of {n_reduce}")

    if not lambda_run and not args.reduce:
        return

    if cpu_extra > 0 and ram_extra > 0 and net_extra > 0:
        count += 1
        if count < 2:
            tag_ecs_count(cluster, count)
            logger.info(f"Skip terminate count={count}")
        else:
            instance = res["i_id"]
            logger.info(f"Terminate {instance}")
            try:
                CLIENT_AUTOSCALING.terminate_instance_in_auto_scaling_group(
                    InstanceId=instance, ShouldDecrementDesiredCapacity=True
                )
            except Exception as e:
                logger.info(e)
            tag_ecs_count(cluster, 0)
    else:
        tag_ecs_count(cluster, 0)


def do_work():
    if lambda_run or not args.clusters:
        # warning max 100 clusters are returned
        clusters = CLIENT_ECS.list_clusters()["clusterArns"]
    else:
        clusters = args.clusters
    clusters = CLIENT_ECS.describe_clusters(
        clusters=clusters, include=["TAGS", "STATISTICS"]
    )["clusters"]
    for c in clusters:
        process_cluster(c)


def lambda_handler(event=None, context=None):
    global lambda_run
    global args

    lambda_run = True if event else False
    args = get_args()
    do_work()


if __name__ == "__main__":
    lambda_handler()
