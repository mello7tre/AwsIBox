# vim: ft=python
import json
import time
import boto3
import logging
from pprint import pprint, pformat
from datetime import datetime, timedelta

# logging.basicConfig()
logger = logging.getLogger("ECSDrainInstance")
handler = logging.StreamHandler()
logger.addHandler(handler)
logger.setLevel(logging.INFO)

SLEEP_TIME = 30

AWS_MAP = {
    "ecs": "ecs",
    "asg": "autoscaling",
    "clf": "cloudformation",
    "sns": "sns",
}


class aws_client(object):
    def __init__(self):
        setattr(self, "region", boto3.session.Session().region_name)
        pass

    # Remember that __getattr__ is only used for missing attribute lookup
    def __getattr__(self, name):
        try:
            client = boto3.client(AWS_MAP[name])
            setattr(self, name, client)
        except Exception:
            raise AttributeError
        else:
            return getattr(self, name)


aws = aws_client()


def getClfExports():
    exports = {}
    paginator = aws.clf.get_paginator("list_exports")
    responseIterator = paginator.paginate()

    for e in responseIterator:
        for export in e["Exports"]:
            name = export["Name"]
            value = export["Value"]
            exports[name] = value

    return exports


def find_cluster(asg_name):
    asgTags = aws.asg.describe_auto_scaling_groups(AutoScalingGroupNames=[asg_name])[
        "AutoScalingGroups"
    ][0]["Tags"]

    for n in asgTags:
        if n["Key"] == "aws:cloudformation:stack-name":
            stack_name = n["Value"]
            try:
                return getClfExports()[f"Cluster-{stack_name}"]
            except Exception:
                break


def get_heartbeat_timeout(asg_name, lifecyclehook_name):
    try:
        l_hooks = aws.asg.describe_lifecycle_hooks(
            AutoScalingGroupName=asg_name, LifecycleHookNames=[lifecyclehook_name]
        )
        heartbeat_timeout = l_hooks["LifecycleHooks"][0]["HeartbeatTimeout"]
    except Exception:
        heartbeat_timeout = 900
    finally:
        return heartbeat_timeout


def find_container_instance(cluster, instance_id, status=None):
    status_desc = ""
    kwargs = {"cluster": cluster, "filter": f"ec2InstanceId == {instance_id}"}

    if status:
        kwargs["status"] = status
        status_desc = f" with status {status}"

    list_resp = aws.ecs.list_container_instances(**kwargs)

    try:
        instance_arn = list_resp["containerInstanceArns"][0]
        logger.info(f"Found Instance_Arn {instance_arn}{status_desc}")
        return instance_arn
    except Exception as e:
        logger.warning(f"Could not find Container Instance ID{status_desc}: {e}")
        return False


def get_tasks(cluster, instance_arn, status):
    resp = aws.ecs.list_tasks(
        cluster=cluster, containerInstance=instance_arn, desiredStatus=status
    )
    tasks = resp.get("taskArns", [])
    logger.info(
        f"{len(tasks)} tasks are in status {status} on Instance Arn {instance_arn}"
    )

    return resp.get("taskArns", [])


def send_message(topic_arn, heartbeat_timeout, msg):
    time_format = "%Y-%m-%dT%H:%M:%S.%fZ"

    if "EndTime" in msg:
        if datetime.utcnow() > datetime.strptime(msg["EndTime"], time_format):

            logger.info("End HeartbeatTimeout")
            return
    else:
        endtime = datetime.strptime(msg["Time"], time_format)
        endtime = endtime + timedelta(seconds=heartbeat_timeout)
        msg["EndTime"] = endtime.strftime(time_format)

    logger.info(
        f"Tasks running, sleeping for {SLEEP_TIME}sec before posting to SNS topic {topic_arn}"
    )
    time.sleep(SLEEP_TIME)

    #    sns_resp = aws.sns.publish(
    #        TopicArn=topic_arn,
    #        Message=json.dumps(msg),
    #        Subject="Publishing SNS msg to invoke Lambda again.",
    #    )

    logger.info("Successfully posted msg %s to SNS topic." % (sns_resp["MessageId"]))


def complete_lifecycle(asg_name, lifecyclehook_name, instance_id):
    logger.info("No tasks running, completing lifecycle")

    aws.asg.complete_lifecycle_action(
        LifecycleHookName=lifecyclehook_name,
        AutoScalingGroupName=asg_name,
        LifecycleActionResult="CONTINUE",
        InstanceId=instance_id,
    )


def drain_instance(cluster, instance_id, instance_arn):
    if find_container_instance(cluster, instance_id, "DRAINING"):
        return True

    logger.info(f"Setting container instance {instance_arn} to DRAINING")
    try:
        resp_update = aws.ecs.update_container_instances_state(
            cluster=cluster, containerInstances=[instance_arn], status="DRAINING"
        )
        instances = resp_update.get("containerInstances")
    except Exception as e:
        logger.error(e)
    else:
        if resp_update.get("failures"):
            logger.error(pformat(resp_update["failures"]))
        elif instances:
            return instances[0]["status"] == "DRAINING"


def tasks_are_running(cluster, tasks_arn, running=0):
    resp = aws.ecs.describe_tasks(cluster=cluster, tasks=tasks_arn)
    tasks = resp.get("tasks", [])

    for t in tasks:
        arn = t["taskArn"]
        status = t["lastStatus"]
        stopped = t.get("stoppedAt")
        logger.info(f"{arn} in {status} [{stopped}]")
        if not stopped:
            running += 1

    if running != 0:
        logger.info(f"{running} task are still running")
        return True


def lambda_handler(event, context):
    topic_arn = event["Records"][0]["Sns"]["TopicArn"]
    msg = json.loads(event["Records"][0]["Sns"]["Message"])

    if "autoscaling:EC2_INSTANCE_TERMINATING" not in msg.get("LifecycleTransition", []):
        return

    asg_name = msg["AutoScalingGroupName"]
    instance_id = msg["EC2InstanceId"]
    lifecyclehook_name = msg["LifecycleHookName"]

    # add prefix to log messages
    formatter = logging.Formatter(
        fmt=f"%(levelname)s:[{instance_id}@{asg_name}]:%(message)s"
    )
    handler.setFormatter(formatter)

    need_to_send_msg = False

    if msg.get("ALREADY_DRAINED"):
        # Workflow triggered by msg sent because tasks are still running on instance
        cluster = msg["Cluster"]
        tasks = msg["Tasks"]
        if tasks_are_running(cluster, tasks):
            need_to_send_msg = True
    else:
        # Workflow triggered by ASG LifeCycleHook
        cluster = find_cluster(asg_name)
        instance_arn = find_container_instance(cluster, instance_id)

        if cluster and instance_arn:
            if drain_instance(cluster, instance_id, instance_arn):
                # container instance is in DRAINING status
                runnig_tasks = get_tasks(cluster, instance_arn, "RUNNING")
                if runnig_tasks:
                    msg["ALREADY_DRAINED"] = True
                    msg["Cluster"] = cluster
                    msg["Tasks"] = runnig_tasks
                    need_to_send_msg = True
            else:
                need_to_send_msg = True
        else:
            logger.warning(f"Unable to find Cluster and Instance Arn")
            return

    if need_to_send_msg:
        heartbeat_timeout = get_heartbeat_timeout(asg_name, lifecyclehook_name)
        logger.info(f"HeartBeatTimeout: {heartbeat_timeout}")
        send_message(topic_arn, heartbeat_timeout, msg)
    else:
        complete_lifecycle(asg_name, lifecyclehook_name, instance_id)



# test by cmd line
if __name__ == "__main__":
    message = {
        "AutoScalingGroupName": "rmq-z-AutoScalingGroup-ZP5HX5UZLDFU",
        "EC2InstanceId": "i-0c5513e2a13d91170",
        "LifecycleTransition": "autoscaling:EC2_INSTANCE_TERMINATING",
        "LifecycleHookName": "rmq-z-ASGLifecycleHookECSDrainInstance-jyqNwkJgVsg8",
        "Time": "2022-04-30T20:20:20.33Z",
    }
    message["ALREADY_DRAINED"] = True
    message["Cluster"] = "rmq-z-Cluster-FbOYhwfK9szD"
    message["Tasks"] = [
        "a312d31b47d446b7ace1c59644724d46",
        "c4bde60907de411b91e6f9780bab3d1a",
    ]
    event = {
        "Records": [
            {
                "Sns": {
                    "TopicArn": "test topic arn",
                    "Message": json.dumps(message),
                }
            }
        ]
    }
    lambda_handler(event, {})
