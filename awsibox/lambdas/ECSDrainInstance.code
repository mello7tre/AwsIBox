# vim: ft=python
import json
import time
import boto3
import os
import logging
from pprint import pprint, pformat
from datetime import datetime, timedelta

# logging.basicConfig()
logger = logging.getLogger("ECSDrainInstance")
handler = logging.StreamHandler()
logger.addHandler(handler)
logger.setLevel(logging.INFO)
logger.propagate = False


AWS_MAP = {
    "ecs": "ecs",
    "asg": "autoscaling",
    "clf": "cloudformation",
    "sns": "sns",
}


class aws_client(object):
    def __init__(self):
        setattr(self, "region", boto3.session.Session().region_name)
        pass

    # Remember that __getattr__ is only used for missing attribute lookup
    def __getattr__(self, name):
        try:
            client = boto3.client(AWS_MAP[name])
            setattr(self, name, client)
        except Exception:
            raise AttributeError
        else:
            return getattr(self, name)


aws = aws_client()


def getClfExports():
    exports = {}
    paginator = aws.clf.get_paginator("list_exports")
    responseIterator = paginator.paginate()

    for e in responseIterator:
        for export in e["Exports"]:
            name = export["Name"]
            value = export["Value"]
            exports[name] = value

    return exports


def find_cluster(asg_name):
    asgTags = aws.asg.describe_auto_scaling_groups(AutoScalingGroupNames=[asg_name])[
        "AutoScalingGroups"
    ][0]["Tags"]

    for n in asgTags:
        if n["Key"] == "aws:cloudformation:stack-name":
            stack_name = n["Value"]
            try:
                return getClfExports()[f"Cluster-{stack_name}"]
            except Exception:
                break


def get_heartbeat_timeout(asg_name, lifecyclehook_name):
    heartbeat_timeout = 900

    resp = aws.asg.describe_lifecycle_hooks(
        AutoScalingGroupName=asg_name, LifecycleHookNames=[lifecyclehook_name]
    )

    for l in resp.get("LifecycleHooks", []):
        if "ECSDrainInstance" in l.get("NotificationTargetARN", ""):
            heartbeat_timeout = l["HeartbeatTimeout"]
            break

    return heartbeat_timeout


def find_container_instance(cluster, instance_id, status=None):
    status_desc = ""
    kwargs = {"cluster": cluster, "filter": f"ec2InstanceId == {instance_id}"}

    if status:
        kwargs["status"] = status
        status_desc = f" with status {status}"

    list_resp = aws.ecs.list_container_instances(**kwargs)

    try:
        instance_arn = list_resp["containerInstanceArns"][0]
        logger.info(f"Found {instance_arn}{status_desc}")
        return instance_arn
    except Exception:
        logger.warning(f"Could not find Container Instance{status_desc}")
        return False


def get_tasks(cluster, instance_arn, status):
    tasks = []
    paginator = aws.ecs.get_paginator("list_tasks")
    response_iter = paginator.paginate(
        cluster=cluster, containerInstance=instance_arn, desiredStatus=status
    )
    for resp in response_iter:
        for t in resp.get("taskArns", []):
            tasks.append(os.path.basename(t))

    logger.info(f"{len(tasks)} tasks are in status {status} on {instance_arn}")

    return tasks


def send_message(topic_arn, heartbeat_timeout, msg):
    time_format = "%Y-%m-%dT%H:%M:%S.%fZ"

    if "EndTime" in msg:
        # subsequent sleep time cheking for tasks to stop
        sleep_time = 20
        endtime = datetime.strptime(msg["EndTime"], time_format)
        if datetime.utcnow() > endtime:
            logger.info(f"HeartbeatTimeout Ended: {datetime.utcnow()} > {endtime}")
            return
    else:
        # initial sleep time after drainig container
        sleep_time = 40
        endtime = datetime.strptime(msg["Time"], time_format) + timedelta(
            seconds=heartbeat_timeout
        )
        msg["EndTime"] = endtime.strftime(time_format)
        logger.info(f"HeartbeatTimeout Setting EndTime: {endtime}")

    logger.info(f"Sleeping for {sleep_time}sec ... posting to SNS {topic_arn}")
    time.sleep(sleep_time)

    sns_resp = aws.sns.publish(
        TopicArn=topic_arn,
        Message=json.dumps(msg),
        Subject="Publishing SNS msg to invoke Lambda again.",
    )

    logger.info("Posted msg %s " % (sns_resp["MessageId"]))


def complete_lifecycle(asg_name, lifecyclehook_name, instance_id):
    logger.info("No tasks running, completing lifecycle")

    aws.asg.complete_lifecycle_action(
        LifecycleHookName=lifecyclehook_name,
        AutoScalingGroupName=asg_name,
        LifecycleActionResult="CONTINUE",
        InstanceId=instance_id,
    )


def drain_instance(cluster, instance_arn):
    logger.info(f"Setting container instance {instance_arn} to DRAINING")
    try:
        resp_update = aws.ecs.update_container_instances_state(
            cluster=cluster, containerInstances=[instance_arn], status="DRAINING"
        )
        instances = resp_update.get("containerInstances")
    except Exception as e:
        logger.error(e)
    else:
        if resp_update.get("failures"):
            logger.error(pformat(resp_update["failures"]))
        elif instances:
            return instances[0]["status"] == "DRAINING"


def tasks_are_running(cluster, tasks_arn, running=0):
    while tasks_arn:
        tasks_arn_sub = tasks_arn[0:100]
        resp = aws.ecs.describe_tasks(cluster=cluster, tasks=tasks_arn_sub)
        tasks = resp.get("tasks", [])

        for t in tasks:
            arn = t["taskArn"]
            status = t["lastStatus"]
            stopped = t.get("stoppedAt")
            logger.debug(f"{cluster} {arn} in {status} [{stopped}]")
            if not stopped:
                running += 1
        del tasks_arn[0:100]

    if running != 0:
        logger.info(f"{running} task are still running")
        return True


def lambda_handler(event, context):
    topic_arn = event["Records"][0]["Sns"]["TopicArn"]
    msg = json.loads(event["Records"][0]["Sns"]["Message"])

    if "autoscaling:EC2_INSTANCE_TERMINATING" not in msg.get("LifecycleTransition", []):
        return

    asg_name = msg["AutoScalingGroupName"]
    instance_id = msg["EC2InstanceId"]
    lifecyclehook_name = msg["LifecycleHookName"]

    # add prefix to log messages
    formatter = logging.Formatter(
        fmt=f"%(levelname)s:[{instance_id}@{asg_name}]:%(message)s"
    )
    handler.setFormatter(formatter)

    need_to_send_msg = False

    if msg.get("ALREADY_DRAINED"):
        # Workflow triggered by msg sent because tasks are still running on instance
        cluster = msg["Cluster"]
        tasks = msg["Tasks"]
        if tasks_are_running(cluster, tasks):
            need_to_send_msg = True
    else:
        # Workflow triggered by ASG LifeCycleHook
        cluster = find_cluster(asg_name)
        instance_arn = find_container_instance(cluster, instance_id)

        if cluster and instance_arn:
            if find_container_instance(
                cluster, instance_id, "DRAINING"
            ) or drain_instance(cluster, instance_arn):
                # container instance is in DRAINING status
                runnig_tasks = get_tasks(cluster, instance_arn, "RUNNING")
                if runnig_tasks:
                    msg["ALREADY_DRAINED"] = True
                    msg["Cluster"] = cluster
                    msg["Tasks"] = runnig_tasks
                    need_to_send_msg = True
            else:
                need_to_send_msg = True
        else:
            logger.warning(f"Unable to find Cluster and Instance Arn")
            return

    if need_to_send_msg:
        heartbeat_timeout = get_heartbeat_timeout(asg_name, lifecyclehook_name)
        logger.info(f"HeartBeatTimeout: {heartbeat_timeout}")
        send_message(topic_arn, heartbeat_timeout, msg)
    else:
        complete_lifecycle(asg_name, lifecyclehook_name, instance_id)


# test by cmd line
# if __name__ == "__main__":
#    message = {
#        "AutoScalingGroupName": "rmq-z-AutoScalingGroup-ZP5HX5UZLDFU",
#        "EC2InstanceId": "i-04208828b4cd76ec1",
#        "LifecycleTransition": "autoscaling:EC2_INSTANCE_TERMINATING",
#        "LifecycleHookName": "rmq-z-ASGLifecycleHookECSDrainInstance-jyqNwkJgVsg8",
#        "Time": "2022-03-30T20:20:20.33Z",
#    }
#    # uncomment to test re-sent message workflow
#    #message["ALREADY_DRAINED"] = True
#    #message["EndTime"] = "2022-03-30T20:20:20.33Z"
#    #
#    message["Cluster"] = "rmq-z-Cluster-FbOYhwfK9szD"
#    message["Tasks"] = [
#        "38a0a5b76a744a829a7da5082764276a",
#        "da0501ef38994fe28f0b9a864915aefd",
#    ]
#    event = {
#        "Records": [
#            {
#                "Sns": {
#                    "TopicArn": "test topic arn",
#                    "Message": json.dumps(message),
#                }
#            }
#        ]
#    }
#    lambda_handler(event, {})
